{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Esplorare_dati.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Esplorare i dati"
      ],
      "metadata": {
        "id": "kJjZ3VFiACCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando ci interessiamo a un problema e vogliamo rispondere a delle domande specifiche in maniera **quantitativa** attraverso l'analisi di dati, per prima cosa dobbiamo chiederci che tipo di dati relativi al problema sono disponibili, se dobbiamo occuparci noi della raccolta e selezione (aggregazione e \"pulizia\" dei dati) o se qualcuno, tipicamente un'Istituto di ricerca o un'ente pubblico o un soggetto privato (ad es Google), ha già fatto il lavoro per noi e ci mette a disposizione in rete una _base di dati_, di solito chiamato _Dataset_. \n",
        "\n",
        "I _Dataset_ possono variare per: \n",
        "\n",
        "1. **dimensioni**, da una tabella di qualche decina di righe e colonne fino a raggiungere una scala di TeraByte di informazioni (1 TB = 1 _bilione_ di byte). Le grandi basi di dati richiedono importanti risorse e strumenti dedicati, e generalmente vengono indicate come **Big Data**. Possiamo analizzare i dati tutti insieme oppure solo una porzione selezionata secondo alcuni criteri scelti da noi in modo che rappresenti un _campione statisticamente significativo_, e assumere che sia rappresentativo di tutto l'insieme di dati. Una volta le risorse tecnologiche permettevano di analizzare solo un campione dei dati, mentre le tecnologie Big Data ora permettono di fare analisi su **tutti** i dati in nostro possesso.\n",
        "\n",
        "2. **struttura e accessibilità**, cioè il modo in cui vengono strutturati e resi accessibili: ormai tutti i Dataset sono archiviati su supporto digitale. Per essere analizzati, i dati devono essere trasferiti dall'unità di archiviazione (hard disk o Cloud) alla memoria RAM del computer, che per i personal computer di solito è dell'ordine di qualche GB. Per questo motivo le grandi basi di dati non possono essere archiviate in un unico gigantesco file e caricate nella RAM, ma vengono strutturate e organizzate da software dedicati, i _database_, che si occupano di organizzare i dati in maniera efficiente ed effettuare ricerche al loro interno su richiesta di un utente esterno. L'utente si interfaccia col database tramite un linguaggio dedicato (uno dei più comuni si chiama SQL) e richiede un'operazione di ricerca, una **Query**. Nel PCTO non lavoreremo né sui Big Data né sui database, ma quello che impareremo vale anche in quei casi.\n",
        "\n",
        "3. **formato**, cioè il modo specifico in cui i dati sono organizzati: per righe e colonne, in file leggibili da noi umani (caratteri alfanumerici) oppure solo dal computer (in binario, solo 101010, in questa modalità occupano molto meno spazio su disco). Ogni tipo di formato scelto per un dataset può venire letto da una specifica funzione di una libreria del linguaggio di programmazione (Python). Qui useremo database nel formato \".csv\": i dati sono organizzati nel file per righe e in ogni riga le colonne sono separate da un _delimiter_ un carattere scelto come separatore, dove la prima riga conterrà i nomi delle colonne. Nella libreria Pandas di Python, la funzione che leggerà il file _csv_ del dataset e assegnerà i dati a una struttura dati interna chiamata _dataframe_ (una struttura dati è un'insieme di variabili che si riferiscono alla stessa unità di informazione), si chiamerà _pandas.read_csv()_\n"
      ],
      "metadata": {
        "id": "h4kqwI4oAKTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colleghiamo Colab a GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a9c7ab-7435-4da6-9f87-5c3bac0ef4d8",
        "id": "DN-5ODNGNo0s"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nella prima fase dell'analisi dei dati si dà un'occhiata ai dati, cioè si carica il dataset in memoria e si associa a una struttura dati, si vedono che cosa rappresentano le colonne di dati presenti, si calcolano gli indici statistici descrittivi come media, mediana e deviazione e si visualizza graficamente una porzione dei dati o tutti."
      ],
      "metadata": {
        "id": "VRPqeZjNNzFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per ordine e comodità abbiamo raccolto tutti i Dataset in una cartella all'interno del _repository_ GitHub, a cui potete accedere da Colab se avete completato correttamente la procedura del noteboook `Filesystem_startup.ipynb`.\n",
        "\n",
        "il _path_ della cartella è: `/content/drive/MyDrive/Github/cd5050-LauraBassi/Datasets/`"
      ],
      "metadata": {
        "id": "CQioHow3PfOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assegnamo il path della cartella dei dataset a una variabile stringa di caratteri\n",
        "data_path = '/content/drive/MyDrive/Github/cd5050-LauraBassi/Datasets/'\n",
        "print(data_path)\n",
        "# ora andiamo nella cartella Dataset e guardiamo quali dataset sono disponibili\n",
        "%cd /content/drive/MyDrive/Github/cd5050-LauraBassi/Datasets/\n",
        "# l'istruzione seguente produce una lista di tutti i file che hanno estensione csv\n",
        "# ('*' qui sta per 'qualsiasi nome di file') \n",
        "%ls -la *.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlzdfMVNQf8b",
        "outputId": "73f1c992-efb1-48fa-bcdd-d455323f303f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github/cd5050-LauraBassi/Datasets/\n",
            "/content/drive/MyDrive/Github/cd5050-LauraBassi/Datasets\n",
            "-rw------- 1 root root   49 Feb 22 07:47 csv_toy.csv\n",
            "-rw------- 1 root root 2559 Feb 22 07:47 DATI_ISTAT.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scegliamo un dataset su cui lavorare, per esempio *DATI_ISTAT.csv*"
      ],
      "metadata": {
        "id": "vn-662ZSSSbj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhKKz8iKQcgT"
      },
      "source": [
        "# Segnaliamo che vogliamo utilizzare la libreria pandas\n",
        "import pandas  \n",
        "# Carichiamo il file DATI_ISTAT.csv del dataset in memoria RAM \n",
        "# e mettiamo tutte le informazioni in una struttura dati\n",
        "# a cui assegnamo il nome 'data'                       \n",
        "data = pandas.read_csv('DATI_ISTAT.csv', delimiter=\";\")    \n",
        "# con questo comando stampiamo le prime righe del file (head = testa del file)\n",
        "# da cui vediamo i nomi delle colonne e i primi dati\n",
        "data.head()                             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possiamo vedere che i dati seguono la struttura che abbiamo visto anche sul sito dell'ISTAT.\n",
        "\n",
        "Su ogni riga abbiamo una **ossevazione**, mentre a ogni colonna corrisponde una **variabile** (da non confondere con il concetto di variabile nel coding) misurata.\n",
        "\n"
      ],
      "metadata": {
        "id": "jwgosR0W_w7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per prima cosa, vediamo quante sono le righe del file, cioè le **osservazioni**:"
      ],
      "metadata": {
        "id": "eL1UQsweLqhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Le righe del file sono: \" + str(len(data))) # La funzione len ci dice quante righe sono contenute nel file"
      ],
      "metadata": {
        "id": "WooqQ2E1Lvx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo quali e quante sono le colonne del file, cioè le **variabili**."
      ],
      "metadata": {
        "id": "PiKzsxedLlwr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeueqQkPjANf"
      },
      "source": [
        "print(\"Le colonne del file sono \" + str(len(data.columns)) + \" cioè: \" + str(data.columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per le nostre analisi, spesso guarderemo solo alcune delle variabili del dataset. \n",
        "\n",
        "Per selezionare solo alcune colonne, possiamo utilizzare:"
      ],
      "metadata": {
        "id": "ajewMfDzMsmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selezioniamo solo le colonne \"Regione\", \"ScuoleSuperiori\", \"ClassiScuoleSuperiori\", \"IscrittiScuoleSuperiori\", \"RipetentiScuoleSuperiori\"\n",
        "colonne_interessanti = [\"Regione\", \"ScuoleSuperiori\", \"ClassiScuoleSuperiori\",\n",
        "       \"IscrittiScuoleSuperiori\", \"RipetentiScuoleSuperiori\"]\n",
        "dati_puliti = data[colonne_interessanti]\n",
        "dati_puliti.head()"
      ],
      "metadata": {
        "id": "WJk6jTI0yljh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selezionando un solo campo, possiamo utilizzarlo come fosse una lista. Per esempio, possiamo calcolare:"
      ],
      "metadata": {
        "id": "1iUe1frTNq1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print(\"In ogni regione italiana, ci sono in media \" + str(numpy.mean(dati_puliti[\"ScuoleSuperiori\"])) + \" scuole superiori\")"
      ],
      "metadata": {
        "id": "f75MkAJwNyP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ESERCIZIO:** Per il numero di scuole superiori, calcola anche la mediana, la deviazione standard, e il coefficiente di variazione."
      ],
      "metadata": {
        "id": "htLan8rVON0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrivi il codice qui"
      ],
      "metadata": {
        "id": "E3fCUGMROghf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possiamo anche calcolare il minimo e il massimo del numero di scuole nelle regioni italiane:"
      ],
      "metadata": {
        "id": "MoP11IRhOn8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Il numero minimo di scuole nelle diverse regioni è :\" + str(numpy.min(dati_puliti[\"ScuoleSuperiori\"]))) # Utilizziamo la funzione min"
      ],
      "metadata": {
        "id": "DtZFE9-yOvQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Il numero massimo di scuole nelle diverse regioni è :\" + str(numpy.max(dati_puliti[\"ScuoleSuperiori\"]))) # Utilizziamo la funzione max"
      ],
      "metadata": {
        "id": "bskiZw14O-xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ESERCIZIO:** Provate ora a rappresentare graficamente i dati con i vari tipi di grafici che abbiamo visto a lezione, come lo Scatter Plot, la Pie Plot etc"
      ],
      "metadata": {
        "id": "EpCYrmMOUKEn"
      }
    }
  ]
}